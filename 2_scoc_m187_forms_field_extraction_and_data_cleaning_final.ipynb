{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This script processes data from SCOC's M187 incarcerated individual death forms into a table format (these contain\n",
        "many of the same fields as the DOCCS forms, but have a slightly different format, which is why a separate pipeline is necessary).\n",
        "It first classifies the forms by layout type using a Microsoft Azure AI Document Intelligence model, then, it extracts\n",
        "fields from each form using a Document Intelligence model custom built for each layout type. Finally,\n",
        "following a manual review of the extracted fields, it standardizes and cleans fields that need to be converted\n",
        "to the appropriate types (e.g. float, date, time).\n",
        "\n",
        "To run this code, perform the following steps:\n",
        "1. Install the packages in the cell below on your local machine.\n",
        "2. Insert your Azure blob key, URL, and container name for the M187s in the cell below. Also insert your Azure formrecognizer endpoint and key.\n",
        "3. Upload the M187s as PDFs to your Azure blob container, using a separate file for each M187.\n",
        "4. Create a document extraction model for the M187s. To match the field names used throughout this code pipeline,\n",
        "use the following field names in your FormRecognizer model:\n",
        "\n",
        "        \"DECEASED_NAME\", \"MRB_NUMBER\", \"DIN\", \"NYSID\", \"FACILITY_NAME\", \"FACILITY_CODE_2D\", \"ORI\", \\\n",
        "        \"REPORT_DATE\", \"REPORTING_OFFICIAL_NAME\", \"HEIGHT\", \"WEIGHT\", \"RACE\", \"SEX\", \\\n",
        "        \"SENTENCE\", \"BIRTH_DATE\", \"SENTENCE_DATE\", \"ARREST_CHARGES\", \"DATE_ARREST\", \"DATE_CONVICTION\", \"CONVICTION_CHARGES\", \\\n",
        "        \"HOSPITAL_NAME\", \"CHIEF_ADMIN_OFFICER_NAME\", \"AMBULANCE_RESCUE_SQUAD_NAME\", \"DATETIME_ADMITTED\", \"DATE_OF_LAST_ADMISSION\", \\\n",
        "        \"DEATH_DATETIME\", \"DEATH_DATETIME_REPORTED\", \"TERMINAL_INCIDENT_LOCATION\", \"REPORTED_IMMEDIATE_CAUSE_OF_DEATH\", \"FACILITY_ADMINISTRATORS_REPORT_OF_DEATH_CIRCUMSTANCES\", \\\n",
        "        \"OFFICER_SUPERVISING_DEATH_LOCATION\", \"ASSIGNED_HOUSING_UNIT\", \"HOUSING_UNIT_TYPE\", \\\n",
        "        \"AUTOPSY_DATE\", \"AUTOPSY_TIME\", \"AUTOPSY_LOCATION\", \"MEDICAL_EXAMINER_CORONER_NAME\", \"AUTOPSY_PERFORMED_YES\", \"AUTOPSY_PERFORMED_NO\", \\\n",
        "        \"SUPERVISION_PRIOR_TO_INCIDENT\", \"SUBSTANCE_ABUSE_DRUG\", \"SUBSTANCE_ABUSE_ALCOHOL\", \"SUBSTANCE_ABUSE_UNKNOWN\", \"SUBSTANCE_ABUSE_NO\", \\\n",
        "        \"MEDICAL_TREATMENT\", \"PSYCH_TREATMENT\", \"NO_TREATMENT\", \"MEDICAL_CONTACT\", \"PSYCH_CONTACT\", \\\n",
        "        \"INTAKE_SCREENING_YES\", \"INTAKE_SCREENING_NO\"\n",
        "\n",
        "  Fill in the name of your model under # model IDs in the cell below.\n",
        "'''"
      ],
      "metadata": {
        "id": "3Et4RBREhppM"
      },
      "id": "3Et4RBREhppM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0b66ce",
      "metadata": {
        "id": "1b0b66ce"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime as dt\n",
        "import pyodbc\n",
        "import time\n",
        "\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "from azure.storage.blob import BlobClient\n",
        "from azure.storage.blob import BlobServiceClient, ContentSettings\n",
        "from azure.storage.blob import ContainerClient\n",
        "\n",
        "# Azure account info\n",
        "AZURE_BLOB_ACCOUNT_KEY = \"[INSERT ACCOUNT KEY HERE]\"\n",
        "AZURE_BLOB_ACCOUNT_URL = \"[INSERT ACCOUNT URL HERE]\"\n",
        "AZURE_FORM_RECOGNIZER_ENDPOINT = \"[INSERT FORMRECOGNIZER ENDPOINT HERE]\"\n",
        "AZURE_FORM_RECOGNIZER_KEY = \"[INSERT FORMRECOGNIZER KEY HERE]\"\n",
        "AZURE_BLOB_M187_CONTAINER = \"[INSERT CONTAINER NAME HERE]\"\n",
        "\n",
        "document_analysis_client = DocumentAnalysisClient(\n",
        "    endpoint=AZURE_FORM_RECOGNIZER_ENDPOINT, credential=AzureKeyCredential(AZURE_FORM_RECOGNIZER_KEY)\n",
        ")\n",
        "\n",
        "# model IDs\n",
        "SCOC_MODEL_ID = \"[INSERT MODEL NAME HERE]\"\n",
        "\n",
        "# variable lists\n",
        "RAW_DATE_COL_LIST = [\"REPORT_DATE\", \"BIRTH_DATE\", \"SENTENCE_DATE\", \"DATE_ARREST\", \"DATE_CONVICTION\", \"DATE_OF_LAST_ADMISSION\", \"AUTOPSY_DATE\"]\n",
        "RAW_TIME_COL_LIST = [\"AUTOPSY_TIME\"]\n",
        "RAW_DATETIME_COL_LIST = [\"DATETIME_ADMITTED\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94db411a",
      "metadata": {
        "id": "94db411a"
      },
      "outputs": [],
      "source": [
        "# reads names of M187 forms to process\n",
        "def read_file_names_from_blob_container():\n",
        "   '''\n",
        "    Returns the list of file names in an Azure blob container.\n",
        "   '''\n",
        "\n",
        "    # gen empty lists for filenames\n",
        "    FILE_LIST = []\n",
        "\n",
        "    # connect to M187 forms blob container\n",
        "    container = ContainerClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=AZURE_BLOB_M187_CONTAINER, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "\n",
        "    # make list of blobs in container\n",
        "    blob_list = container.list_blobs()\n",
        "\n",
        "    # organize file names of PDF files from blob container\n",
        "    for blob in blob_list:\n",
        "        if (blob.name).split('.')[-1].lower() == 'pdf':\n",
        "            FILE_LIST.append(blob.name)\n",
        "\n",
        "    return FILE_LIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07a93c0",
      "metadata": {
        "id": "b07a93c0"
      },
      "outputs": [],
      "source": [
        "def delete_blob_from_blob_container(blob_name):\n",
        "    '''\n",
        "    Deletes an Azure blob from Azure blob container.\n",
        "    '''\n",
        "\n",
        "    # connect to M187 blob container\n",
        "    container = ContainerClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=AZURE_BLOB_M187_CONTAINER,credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "\n",
        "    # delete blob\n",
        "    container.delete_blob(blob=blob_name)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56a31a0",
      "metadata": {
        "id": "a56a31a0"
      },
      "outputs": [],
      "source": [
        "def save_csv_to_blob_container(df, file_name):\n",
        "    '''\n",
        "    Saves a Pandas DataFrame to an Azure blob container.\n",
        "    '''\n",
        "\n",
        "    # create the BlobServiceClient object\n",
        "    blob_service_client = BlobServiceClient(AZURE_BLOB_ACCOUNT_URL, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "    blob_client = blob_service_client.get_blob_client(container=AZURE_BLOB_M187_CONTAINER, blob=file_name)\n",
        "    blob_settings = ContentSettings(content_encoding='UTF-8')\n",
        "\n",
        "    # save file as blob to container, cool storage\n",
        "    blob_client.upload_blob(df.to_csv(encoding='UTF-8', index=False, sep='|'),overwrite=True,content_type=\"text/csv\")\n",
        "    blob_client.set_standard_blob_tier('Cool')\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The following functions  extract fields from M187s using your custom Azure Document Intelligence model.\n",
        "'''"
      ],
      "metadata": {
        "id": "GjREjaOjmbXa"
      },
      "id": "GjREjaOjmbXa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a2d7c21",
      "metadata": {
        "id": "7a2d7c21"
      },
      "outputs": [],
      "source": [
        "def extract_info_from_scoc_form(file, fields_values_df):\n",
        "    '''\n",
        "    Uses a custom Azure Document Intelligence model to extract fields from an individual form.\n",
        "    '''\n",
        "\n",
        "    model_id = os.getenv(\"SCOC_MODEL_ID\", SCOC_MODEL_ID)\n",
        "    blob_client = BlobClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=\"m187-scoc-forms-blob\", blob_name=file, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "\n",
        "    poller = document_analysis_client.begin_analyze_document_from_url(\n",
        "             model_id = SCOC_MODEL_ID, document_url=blob_client.url\n",
        "         )\n",
        "    result = poller.result()\n",
        "\n",
        "    key_value_dict = {}\n",
        "    key_confidence_dict = {}\n",
        "\n",
        "    for document in result.documents:\n",
        "        key_value_dict[\"FORM_TYPE\"] = \"SCOC\"\n",
        "        key_value_dict[\"FORM_URL\"] = blob_client.url\n",
        "        for field_name, field in document.fields.items():\n",
        "                key_value_dict[field_name] = field.value\n",
        "                key_confidence_dict[field_name] = field.confidence\n",
        "        key_value_dict[\"FIELD_CONFIDENCE_DICT\"] = key_confidence_dict\n",
        "        temp_df = pd.DataFrame([key_value_dict])\n",
        "        fields_values_df = pd.concat([fields_values_df, temp_df], ignore_index=True)\n",
        "\n",
        "    return fields_values_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5df3ba",
      "metadata": {
        "id": "de5df3ba"
      },
      "outputs": [],
      "source": [
        "def extract_info_from_m187_forms(file_list, only_process_new_files, current_fields_values_df=None):\n",
        "    '''\n",
        "    Uses extract_info_from_scoc_form() to extract fields from all files in file list. Returns dataframe with extracted\n",
        "    fields. Note that current_fields_values_df will be set to existing dataframe only if only_process_new_files = True.\n",
        "    '''\n",
        "\n",
        "    # if we want to process all files, create a new empty dataframe for fields and values\n",
        "    if only_process_new_files == False:\n",
        "        fields_values_df = pd.DataFrame(columns = [\"POTENTIAL_DUPLICATE\", \"FILE_NAME\", \"FORM_TYPE\", \"FORM_URL\", \"MANUAL_REVIEW_DONE\", \"DECEASED_NAME\", \"MRB_NUMBER\", \"DIN\", \"NYSID\", \"FACILITY_NAME\", \"FACILITY_CODE_2D\", \"ORI\", \\\n",
        "        \"REPORT_DATE\", \"REPORTING_OFFICIAL_NAME\", \"HEIGHT\", \"WEIGHT\", \"RACE\", \"SEX\", \\\n",
        "        \"SENTENCE\", \"BIRTH_DATE\", \"SENTENCE_DATE\", \"ARREST_CHARGES\", \"DATE_ARREST\", \"DATE_CONVICTION\", \"CONVICTION_CHARGES\", \\\n",
        "        \"HOSPITAL_NAME\", \"CHIEF_ADMIN_OFFICER_NAME\", \"AMBULANCE_RESCUE_SQUAD_NAME\", \"DATETIME_ADMITTED\", \"DATE_OF_LAST_ADMISSION\", \\\n",
        "        \"DEATH_DATETIME\", \"DEATH_DATETIME_REPORTED\", \"TERMINAL_INCIDENT_LOCATION\", \"REPORTED_IMMEDIATE_CAUSE_OF_DEATH\", \"FACILITY_ADMINISTRATORS_REPORT_OF_DEATH_CIRCUMSTANCES\", \\\n",
        "        \"OFFICER_SUPERVISING_DEATH_LOCATION\", \"ASSIGNED_HOUSING_UNIT\", \"HOUSING_UNIT_TYPE\", \\\n",
        "        \"AUTOPSY_DATE\", \"AUTOPSY_TIME\", \"AUTOPSY_LOCATION\", \"MEDICAL_EXAMINER_CORONER_NAME\", \"AUTOPSY_PERFORMED_YES\", \"AUTOPSY_PERFORMED_NO\", \\\n",
        "        \"SUPERVISION_PRIOR_TO_INCIDENT\", \"SUBSTANCE_ABUSE_DRUG\", \"SUBSTANCE_ABUSE_ALCOHOL\", \"SUBSTANCE_ABUSE_UNKNOWN\", \"SUBSTANCE_ABUSE_NO\", \\\n",
        "        \"MEDICAL_TREATMENT\", \"PSYCH_TREATMENT\", \"NO_TREATMENT\", \"MEDICAL_CONTACT\", \"PSYCH_CONTACT\", \\\n",
        "        \"INTAKE_SCREENING_YES\", \"INTAKE_SCREENING_NO\", \"FIELD_CONFIDENCE_DICT\"])\n",
        "\n",
        "    # if we only want to process new values, set fields_values_df to current dataframe so we can append to it\n",
        "    elif only_process_new_files == True:\n",
        "        fields_values_df = current_fields_values_df\n",
        "\n",
        "    # run field extraction models on each forms\n",
        "    for file in file_list:\n",
        "        fields_values_df = extract_info_from_scoc_form(file, fields_values_df)\n",
        "\n",
        "    return fields_values_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The following functions standardize and clean the data once it has been extracted using the Document Intelligence model.\n",
        "They are called once prior to the manual review, and once afterwards.\n",
        "'''"
      ],
      "metadata": {
        "id": "HOokaQsb1XnN"
      },
      "id": "HOokaQsb1XnN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c31ae3",
      "metadata": {
        "id": "f9c31ae3"
      },
      "outputs": [],
      "source": [
        "def clean_doccs_facility_name(df):\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ADIRONDACK'), 'FACILITY_NAME'] = 'ADIRONDACK'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ALBION'), 'FACILITY_NAME'] = 'ALBION'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ALTONA'), 'FACILITY_NAME'] = 'ALTONA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ARTHUR KILL'), 'FACILITY_NAME'] = 'ARTHUR KILL'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ATTICA'), 'FACILITY_NAME'] = 'ATTICA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('AUBURN'), 'FACILITY_NAME'] = 'AUBURN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BAYVIEW'), 'FACILITY_NAME'] = 'BAYVIEW'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BARE HILL'), 'FACILITY_NAME'] = 'BARE HILL'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BEACON'), 'FACILITY_NAME'] = 'BEACON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BEDFORD HILLS'), 'FACILITY_NAME'] = 'BEDFORD HILLS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BUFFALO'), 'FACILITY_NAME'] = 'BUFFALO'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BUTLER'), 'FACILITY_NAME'] = 'BUTLER'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP GABRIELS'), 'FACILITY_NAME'] = 'CAMP GABRIELS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP GEORGETOWN'), 'FACILITY_NAME'] = 'CAMP GEORGETOWN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP PHARSALIA'), 'FACILITY_NAME'] = 'CAMP PHARSALIA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAPE VINCENT'), 'FACILITY_NAME'] = 'CAPE VINCENT'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAYUGA'), 'FACILITY_NAME'] = 'CAYUGA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CHATEAUGAY'), 'FACILITY_NAME'] = 'CHATEAUGAY'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CLINTON'), 'FACILITY_NAME'] = 'CLINTON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('COLLINS'), 'FACILITY_NAME'] = 'COLLINS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('COXSACKIE'), 'FACILITY_NAME'] = 'COXSACKIE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('DOWNSTATE'), 'FACILITY_NAME'] = 'DOWNSTATE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('EASTERN'), 'FACILITY_NAME'] = 'EASTERN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('EDGECOMBE'), 'FACILITY_NAME'] = 'EDGECOMBE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ELMIRA'), 'FACILITY_NAME'] = 'ELMIRA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FISHKILL'), 'FACILITY_NAME'] = 'FISHKILL'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FIVE POINTS'), 'FACILITY_NAME'] = 'FIVE POINTS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FRANKLIN'), 'FACILITY_NAME'] = 'FRANKLIN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FULTON'), 'FACILITY_NAME'] = 'FULTON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GOWANDA'), 'FACILITY_NAME'] = 'GOWANDA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GOUVERNEUR'), 'FACILITY_NAME'] = 'GOUVERNEUR'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREAT MEADOW'), 'FACILITY_NAME'] = 'GREAT MEADOW'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREEN HAVEN'), 'FACILITY_NAME'] = 'GREEN HAVEN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREENE'), 'FACILITY_NAME'] = 'GREENE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GROVELAND'), 'FACILITY_NAME'] = 'GROVELAND'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('HALE CREEK'), 'FACILITY_NAME'] = 'HALE CREEK'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('HUDSON'), 'FACILITY_NAME'] = 'HUDSON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LAKEVIEW'), 'FACILITY_NAME'] = 'LAKEVIEW'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LINCOLN'), 'FACILITY_NAME'] = 'LINCOLN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LIVINGSTON'), 'FACILITY_NAME'] = 'LIVINGSTON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LYON MOUNTAIN'), 'FACILITY_NAME'] = 'LYON MOUNTAIN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MARCY'), 'FACILITY_NAME'] = 'MARCY'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MID-ORANGE'), 'FACILITY_NAME'] = 'MID-ORANGE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MID-STATE'), 'FACILITY_NAME'] = 'MID-STATE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MOHAWK'), 'FACILITY_NAME'] = 'MOHAWK'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MONTEREY'), 'FACILITY_NAME'] = 'MONTEREY'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MORIAH'), 'FACILITY_NAME'] = 'MORIAH'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MOUNT MCGREGOR'), 'FACILITY_NAME'] = 'MOUNT MCGREGOR'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('OGDENSBURG'), 'FACILITY_NAME'] = 'OGDENSBURG'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ONEIDA'), 'FACILITY_NAME'] = 'ONEIDA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ORLEANS'), 'FACILITY_NAME'] = 'ORLEANS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('OTISVILLE'), 'FACILITY_NAME'] = 'OTISVILLE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('QUEENSBORO'), 'FACILITY_NAME'] = 'QUEENSBORO'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('RIVERVIEW'), 'FACILITY_NAME'] = 'RIVERVIEW'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ROCHESTER'), 'FACILITY_NAME'] = 'ROCHESTER'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SHAWANGUNK'), 'FACILITY_NAME'] = 'SHAWANGUNK'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SING SING'), 'FACILITY_NAME'] = 'SING SING'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SOUTHPORT'), 'FACILITY_NAME'] = 'SOUTHPORT'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SULLIVAN'), 'FACILITY_NAME'] = 'SULLIVAN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SUMMIT'), 'FACILITY_NAME'] = 'SUMMIT'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('TACONIC'), 'FACILITY_NAME'] = 'TACONIC'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ULSTER'), 'FACILITY_NAME'] = 'ULSTER'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('UPSTATE'), 'FACILITY_NAME'] = 'UPSTATE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WALSH'), 'FACILITY_NAME'] = 'WALSH'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WALLKILL'), 'FACILITY_NAME'] = 'WALLKILL'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WATERTOWN'), 'FACILITY_NAME'] = 'WATERTOWN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WASHINGTON'), 'FACILITY_NAME'] = 'WASHINGTON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WENDE'), 'FACILITY_NAME'] = 'WENDE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WILLARD'), 'FACILITY_NAME'] = 'WILLARD'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WOODBOURNE'), 'FACILITY_NAME'] = 'WOODBOURNE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WYOMING'), 'FACILITY_NAME'] = 'WYOMING'\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7302d433",
      "metadata": {
        "id": "7302d433"
      },
      "outputs": [],
      "source": [
        "def create_facility_code_2d(df):\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ADIRONDACK'), 'FACILITY_CODE_2D'] = '23'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ALBION'), 'FACILITY_CODE_2D'] = '09'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ALTONA'), 'FACILITY_CODE_2D'] = '54'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ARTHUR KILL'), 'FACILITY_CODE_2D'] = '15'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ATTICA'), 'FACILITY_CODE_2D'] = '00'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('AUBURN'), 'FACILITY_CODE_2D'] = '01'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BARE HILL'), 'FACILITY_CODE_2D'] = '56'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BAYVIEW'), 'FACILITY_CODE_2D'] = '31'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BEACON'), 'FACILITY_CODE_2D'] = '34'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BEDFORD HILLS'), 'FACILITY_CODE_2D'] = '12'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BUFFALO'), 'FACILITY_CODE_2D'] = '88'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BUTLER'), 'FACILITY_CODE_2D'] = '52'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP GABRIELS'), 'FACILITY_CODE_2D'] = '22'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP GEORGETOWN'), 'FACILITY_CODE_2D'] = '21'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP PHARSALIA'), 'FACILITY_CODE_2D'] = '18'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAPE VINCENT'), 'FACILITY_CODE_2D'] = '58'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAYUGA'), 'FACILITY_CODE_2D'] = '55'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CHATEAUGAY'), 'FACILITY_CODE_2D'] = '86'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CLINTON'), 'FACILITY_CODE_2D'] = '02'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('COLLINS'), 'FACILITY_CODE_2D'] = '47'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('COXSACKIE'), 'FACILITY_CODE_2D'] = '13'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('DOWNSTATE'), 'FACILITY_CODE_2D'] = '24'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('EASTERN'), 'FACILITY_CODE_2D'] = '10'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('EDGECOMBE'), 'FACILITY_CODE_2D'] = '32'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ELMIRA'), 'FACILITY_CODE_2D'] = '11'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FISHKILL'), 'FACILITY_CODE_2D'] = '05'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FIVE POINT'), 'FACILITY_CODE_2D'] = '37'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FRANKLIN'), 'FACILITY_CODE_2D'] = '53'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FULTON'), 'FACILITY_CODE_2D'] = '38'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GOWANDA'), 'FACILITY_CODE_2D'] = '45'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GOUVERNEUR'), 'FACILITY_CODE_2D'] = '81'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREAT MEADOW'), 'FACILITY_CODE_2D'] = '04'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREEN HAVEN'), 'FACILITY_CODE_2D'] = '08'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREENE'), 'FACILITY_CODE_2D'] = '67'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GROVELAND'), 'FACILITY_CODE_2D'] = '46'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('HALE CREEK'), 'FACILITY_CODE_2D'] = '85'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('HUDSON'), 'FACILITY_CODE_2D'] = '27'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LAKEVIEW'), 'FACILITY_CODE_2D'] = '60'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LINCOLN'), 'FACILITY_CODE_2D'] = '36'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LIVINGSTON'), 'FACILITY_CODE_2D'] = '80'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LYON MOUNTAIN'), 'FACILITY_CODE_2D'] = '59'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MARCY'), 'FACILITY_CODE_2D'] = '49'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MID-ORANGE'), 'FACILITY_CODE_2D'] = '28'\n",
        "    df.loc[(df['FACILITY_NAME'].str.contains('MIDSTATE'))|(df['FACILITY_NAME'].str.contains('MID-STATE')), 'FACILITY_CODE_2D'] = '48'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MOHAWK'), 'FACILITY_CODE_2D'] = '39'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WALSH'), 'FACILITY_CODE_2D'] = '39'  # make Walsh have same facility code as Mohawk\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MONTEREY'), 'FACILITY_CODE_2D'] = '19'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MORIAH'), 'FACILITY_CODE_2D'] = '51'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MCGREGOR'), 'FACILITY_CODE_2D'] = '26'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('OGDENSBURG'), 'FACILITY_CODE_2D'] = '35'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ONEIDA'), 'FACILITY_CODE_2D'] = '44'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ORLEANS'), 'FACILITY_CODE_2D'] = '64'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('OTISVILLE'), 'FACILITY_CODE_2D'] = '29'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('QUEENSBORO'), 'FACILITY_CODE_2D'] = '17'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('RIVERVIEW'), 'FACILITY_CODE_2D'] = '57'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ROCHESTER'), 'FACILITY_CODE_2D'] = '30'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SHAWANGUNK'), 'FACILITY_CODE_2D'] = '68'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SING SING'), 'FACILITY_CODE_2D'] = '07'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SOUTHPORT'), 'FACILITY_CODE_2D'] = '63'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SULLIVAN'), 'FACILITY_CODE_2D'] = '69'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SUMMIT'), 'FACILITY_CODE_2D'] = '20'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('TACONIC'), 'FACILITY_CODE_2D'] = '25'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ULSTER'), 'FACILITY_CODE_2D'] = '61'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('UPSTATE'), 'FACILITY_CODE_2D'] = '84'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WALLKILL'), 'FACILITY_CODE_2D'] = '06'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WATERTOWN'), 'FACILITY_CODE_2D'] = '03'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WASHINGTON'), 'FACILITY_CODE_2D'] = '65'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WENDE'), 'FACILITY_CODE_2D'] = '43'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WILLARD'), 'FACILITY_CODE_2D'] = '82'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WOODBOURNE'), 'FACILITY_CODE_2D'] = '14'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WYOMING'), 'FACILITY_CODE_2D'] = '66'\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d66c074",
      "metadata": {
        "id": "6d66c074"
      },
      "outputs": [],
      "source": [
        "def to_upper(df):\n",
        "    df = df.applymap(lambda s: s.upper() if type(s) == str else s)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7ff67e",
      "metadata": {
        "id": "5f7ff67e"
      },
      "outputs": [],
      "source": [
        "def clean_race_col(df):\n",
        "\n",
        "    df[\"RACE\"] = df[\"RACE\"].replace(\"A\", \"ASIAN\")\n",
        "    df[\"RACE\"] = df[\"RACE\"].replace(\"B\", \"BLACK\")\n",
        "    df[\"RACE\"] = df[\"RACE\"].replace(\"W\", \"WHITE\")\n",
        "    df[\"RACE\"] = df[\"RACE\"].replace(\"O\", \"OTHER\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c2bfe9",
      "metadata": {
        "id": "c7c2bfe9"
      },
      "outputs": [],
      "source": [
        "def clean_date_time_cols(df):\n",
        "    for col in RAW_DATE_COL_LIST:\n",
        "        df[col] = pd.to_datetime(df[col],  errors = 'ignore')\n",
        "\n",
        "    for col in RAW_TIME_COL_LIST:\n",
        "        df[col] = pd.to_datetime(df[col], errors = 'ignore')\n",
        "\n",
        "    for col in RAW_DATETIME_COL_LIST:\n",
        "        df[col] = pd.to_datetime(df[col], errors = 'ignore')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab9689b",
      "metadata": {
        "id": "5ab9689b"
      },
      "outputs": [],
      "source": [
        "def clean_remaining_cols(df):\n",
        "\n",
        "    # sentence\n",
        "    df[\"SENTENCE\"] = df[\"SENTENCE\"].str.replace(\"YRS\", \"YEARS\", regex = True)\n",
        "    df[\"SENTENCE\"] = df[\"SENTENCE\"].str.replace(\";\", \", \", regex = True)\n",
        "\n",
        "    # housing unit abbreviations\n",
        "    df = df.replace({'REGIONAL MEDICAL UNIT': 'RMU'}, regex=True)\n",
        "    df = df.replace({'GP': 'GENERAL POPULATION'}, regex=False)\n",
        "    df.loc[df['HOUSING_UNIT_TYPE'].str.contains('GEN'), 'HOUSING_UNIT_TYPE'] = 'GENERAL POPULATION'\n",
        "\n",
        "    # form URL -- make the appropriate phrases lowercase so the PDF will download properly\n",
        "    df[\"FORM_URL\"] = df[\"FORM_URL\"].str.replace(\".PDF\", \".pdf\", regex = True)\n",
        "    df[\"FORM_URL\"] = df[\"FORM_URL\"].str.replace(\"M187-SCOC-FORMS-BLOB\", \"m187-scoc-forms-blob\", regex = True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1dfdeee",
      "metadata": {
        "id": "c1dfdeee"
      },
      "outputs": [],
      "source": [
        "def strip_whitespace(df):\n",
        "    df = df.applymap(lambda s: s.strip() if type(s) == str else s)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21950ad",
      "metadata": {
        "id": "c21950ad"
      },
      "outputs": [],
      "source": [
        "def clean_df(df):\n",
        "    '''\n",
        "    Runs all data cleaning and standardization functions.\n",
        "    '''\n",
        "    df = df.astype(str)\n",
        "    df = df.dropna(how='all')\n",
        "    df = df.replace({'nan': ''}, regex=True)\n",
        "    df = df.replace({'|': ''}, regex=True)\n",
        "    df = df.replace({'\\r': ''}, regex=True)\n",
        "    df = df.replace({'DNA': 'DID NOT ANSWER'}, regex=True)\n",
        "\n",
        "    df = to_upper(df)\n",
        "    df = clean_doccs_facility_name(df)\n",
        "    df = clean_race_col(df)\n",
        "    df = clean_date_time_cols(df)\n",
        "    df = clean_remaining_cols(df)\n",
        "    df = strip_whitespace(df)\n",
        "\n",
        "    df = df.astype(str)\n",
        "    df = df.replace({'NaT': ''}, regex=True)\n",
        "    df = df.replace({'NONE': ''}, regex=True)\n",
        "\n",
        "    # abbreviate checkbox indicators for ease of checking\n",
        "    df = df.replace({'UNSELECTED': ''}, regex=True)\n",
        "    df = df.replace({'SELECTED': 'X'}, regex=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4cf93f",
      "metadata": {
        "id": "fa4cf93f"
      },
      "outputs": [],
      "source": [
        "def process_raw_forms(only_process_new_files):\n",
        "    '''\n",
        "    The main function for before the manual review step. Classifies and extracts information from forms and performs\n",
        "    preliminary data cleaning before uploading to the Azure blob as a CSV. Set only_process_new_files to True if we\n",
        "    only want to process M187s that haven't been processed yet; set to False if we want to drop all processed data\n",
        "    and reprocess all forms.\n",
        "    '''\n",
        "\n",
        "    # read file names from blob container\n",
        "    FILE_LIST = read_file_names_from_blob_container()\n",
        "\n",
        "    # if we only want to process new files, update list only with files which don't already have their fields in the data table\n",
        "    if only_process_new_files == True:\n",
        "        # get CSV with fields extracted from forms so far and turn it into a dataframe so that we can add to it\n",
        "        blob_client = BlobClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=AZURE_BLOB_M187_CONTAINER, blob_name=\"M187_SCOC_extracted_fields_table.csv\", credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "        blob_download = blob_client.download_blob()\n",
        "        blob_content = blob_download.readall().decode('UTF-8')\n",
        "        M187_extracted_fields_df = pd.DataFrame([x.split('|') for x in blob_content.split('\\n')])\n",
        "        M187_extracted_fields_df = M187_extracted_fields_df.replace({'\"FIELD_CONFIDENCE_DICT\\r\"\\r': 'FIELD_CONFIDENCE_DICT'}, regex=True)\n",
        "\n",
        "        # delete old extracted fields df so we can upload a new one later\n",
        "        delete_blob_from_blob_container(\"M187_SCOC_extracted_fields_table.csv\")\n",
        "\n",
        "        # make top row header\n",
        "        header = M187_extracted_fields_df.iloc[0]\n",
        "        M187_extracted_fields_df = M187_extracted_fields_df[1:]\n",
        "        M187_extracted_fields_df.columns = header\n",
        "\n",
        "        processed_file_list = M187_extracted_fields_df[\"FILE_NAME\"].tolist()\n",
        "        processed_file_list = [f for f in processed_file_list if f is not None]\n",
        "        processed_file_list = [f.replace(\"PDF\", \"pdf\") for f in processed_file_list]\n",
        "        processed_file_list = [f.replace(\"PAGES\", \"pages\") for f in processed_file_list]\n",
        "\n",
        "        # drop names of processed files (i.e. files already represented in the extracted fields dataframe, even if they haven't been manually checked yet) from the list\n",
        "        FILE_LIST = [f for f in FILE_LIST if f not in processed_file_list]\n",
        "\n",
        "        print('Files to process:', FILE_LIST)\n",
        "        df = extract_info_from_m187_forms(FILE_LIST, only_process_new_files, current_fields_values_df = M187_extracted_fields_df)\n",
        "\n",
        "    # if we want to process all files, we can ignore the extracted fields dataframe already in the blob\n",
        "    elif only_process_new_files == False:\n",
        "        df[\"MANUAL_REVIEW_DONE\"] = \"NO\"\n",
        "        print('Files to process:', FILE_LIST)\n",
        "        df = extract_info_from_m187_forms(FILE_LIST, only_process_new_files)\n",
        "\n",
        "    df = clean_df(df)\n",
        "    save_csv_to_blob_container(df, \"M187_SCOC_extracted_fields_table.csv\")\n",
        "    print('Processing complete')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43110144",
      "metadata": {
        "id": "43110144"
      },
      "outputs": [],
      "source": [
        "def clean_df_post_manual_check():\n",
        "    '''\n",
        "    The main function for after the manual review step. Performs data QA and final data cleaning before uploading the\n",
        "    cleaned file to the Azure blob as a CSV.\n",
        "    '''\n",
        "    file = \"M187_SCOC_extracted_fields_table.csv\"\n",
        "    blob_client = BlobClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=\"m187-scoc-forms-blob\", blob_name=file, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "    blob_download = blob_client.download_blob()\n",
        "    blob_content = blob_download.readall().decode('UTF-8')\n",
        "    df = pd.DataFrame([x.split('|') for x in blob_content.split('\\n')])\n",
        "\n",
        "    # make top row header\n",
        "    header = df.iloc[0]\n",
        "    df = df[1:]\n",
        "    df.columns = header\n",
        "\n",
        "    # final clean of dataframe\n",
        "    df = clean_df(df)\n",
        "\n",
        "    # check DIN and set to missing if necessary\n",
        "    df.loc[df[\"DIN\"].apply(lambda x: not re.match(\"^\\d{2}[A-Z]\\d{4}$\", x)), \"DIN\"] = \"\"\n",
        "\n",
        "    # add facility code\n",
        "    df = create_facility_code_2d(df)\n",
        "\n",
        "    # delete the old blob from the container so we can upload the new dataframe\n",
        "    delete_blob_from_blob_container(\"M187_SCOC_extracted_fields_table.csv\")\n",
        "\n",
        "    # save cleaned dataframe to container\n",
        "    save_csv_to_blob_container(df, \"M187_SCOC_extracted_fields_table.csv\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02df19cb",
      "metadata": {
        "id": "02df19cb"
      },
      "outputs": [],
      "source": [
        "# main function execution\n",
        "def main(process_data, only_process_new_files, run_post_processing):\n",
        "\n",
        "    if process_data == True:\n",
        "\n",
        "        process_raw_forms(only_process_new_files)\n",
        "\n",
        "\n",
        "    if run_post_processing == True:\n",
        "\n",
        "        clean_data_post_processing()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83337e4c",
      "metadata": {
        "id": "83337e4c"
      },
      "outputs": [],
      "source": [
        "# run settings\n",
        "'''\n",
        "First set process_data to True and run_post_manual_review to False. Set only_process_new_files to either True or False depending on\n",
        "whether to process only new files, or to reprocess all files.\n",
        "\n",
        "Then, perform the MANUAL REVIEW STEP: Manually clean the output data to correct Document Intelligence errors. Flag redactions and keep only one\n",
        "duplicate for each set of duplicates.\n",
        "\n",
        "Finally, set process_data to False and run_post_manual_review to True. This will process the manually reviewed data to standardize the formatting.\n",
        "'''\n",
        "process_data = True # read and transform raw data files\n",
        "\n",
        "run_post_manual_review = False # run data processing after manual review process has been completed\n",
        "\n",
        "main(process_data, only_process_new_files, run_post_processing)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}