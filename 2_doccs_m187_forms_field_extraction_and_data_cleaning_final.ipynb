{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e4e83f",
      "metadata": {
        "id": "48e4e83f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This script processes data from DOCCS' M187 incarcerated individual death forms into a table format. It first\n",
        "classifies the forms by layout type using a Microsoft Azure AI Document Intelligence model, then, it extracts\n",
        "fields from each form using a Document Intelligence model custom built for each layout type. Finally,\n",
        "following a manual review of the extracted fields, it standardizes and cleans fields that need to be converted\n",
        "to the appropriate types (e.g. float, date, time).\n",
        "\n",
        "To run this code, perform the following steps:\n",
        "1. Install the packages in the cell below on your local machine.\n",
        "2. Insert your Azure blob key, URL, and container name for the M187s in the cell below. Also insert your Azure formrecognizer endpoint and key.\n",
        "3. Upload the M187s as PDFs to your Azure blob container, using a separate file for each M187 (first page only).\n",
        "4. Create a document classification model in FormRecognizer, using a few M187 samples to train the model. Also create document extraction\n",
        "models for each M187 type (boldface and non-boldface). To match the field names used throughout this code pipeline, use the following\n",
        "field names in your FormRecognizer models:\n",
        "\n",
        "        \"DECEASED_NAME\", \"DIN\", \"NYSID\", \"FACILITY_NAME\", \"CODE\", \\\n",
        "        \"REPORT_DATE\", \"REPORTING_OFFICIAL_NAME\", \"HEIGHT\", \"HEIGHT_FT\", \"HEIGHT_IN\", \"ETHNICITY\", \"WEIGHT\", \"RACE\", \"SEX\", \\\n",
        "        \"SENTENCE\", \"BIRTH_DATE\", \"SENTENCE_DATE\", \"ARREST_CHARGES\", \"DATE_ARREST\", \"DATE_CONVICTION\", \"CONVICTION_CHARGES\", \\\n",
        "        \"HOSPITAL_NAME\", \"CHIEF_ADMIN_OFFICER_NAME\", \"AMBULANCE_RESCUE_SQUAD_NAME\", \"DATETIME_ADMITTED\", \"DATE_OF_LAST_ADMISSION\", \\\n",
        "        \"DEATH_DATE\", \"DEATH_TIME\", \"TERMINAL_INCIDENT_LOCATION\", \"REPORTED_IMMEDIATE_CAUSE_OF_DEATH\", \"FACILITY_ADMINISTRATORS_REPORT_OF_DEATH_CIRCUMSTANCES\", \\\n",
        "        \"STAFF_INCARCERATED_WITNESSES\", \"OFFICER_SUPERVISING_DEATH_LOCATION\", \"ASSIGNED_HOUSING_UNIT\", \"HOUSING_UNIT_TYPE\", \\\n",
        "        \"AUTOPSY_DATE\", \"AUTOPSY_TIME\", \"AUTOPSY_DATETIME\", \"AUTOPSY_LOCATION\", \"MEDICAL_EXAMINER_CORONER_NAME\", \"AUTOPSY_PERFORMED_YES\", \"AUTOPSY_PERFORMED_NO\", \\\n",
        "        \"SUPERVISION_PRIOR_TO_INCIDENT_ACTIVE\", \"SUPERVISION_PRIOR_TO_INCIDENT_CONSTANT\", \"SUPERVISION_PRIOR_TO_INCIDENT_GENERAL\",\n",
        "        \"SUBSTANCE_ABUSE_DRUG\", \"SUBSTANCE_ABUSE_ALCOHOL\", \"SUBSTANCE_ABUSE_UNKNOWN\", \"SUBSTANCE_ABUSE_NO\", \\\n",
        "        \"MEDICAL_TREATMENT\", \"PSYCH_TREATMENT\", \"NO_TREATMENT\", \"DATE_LAST_CONTACT\", \"MEDICAL_CONTACT\", \"PSYCH_CONTACT\", \\\n",
        "        \"INTAKE_SCREENING_YES\", \"INTAKE_SCREENING_NO\", \"FIELD_CONFIDENCE_DICT\", \"FACILITY_CODE_2D\", \"WEIGHT_LBS_F\", \"DATE_OF_BIRTH_D\", \\\n",
        "        \"DEATH_TIME_T\", \"DATE_ARREST_D\", \"DATE_CONVICTION_D\", \"SENTENCE_DATE_D\", \"AUTOPSY_DATE_D\", \"DATETIME_ADMITTED_DT\", \"DATE_OF_LAST_ADMISSION_D\"\n",
        "\n",
        "5. Once you have created the classification and extraction models, fill in the names of the models under #model IDs in the cell below.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0b66ce",
      "metadata": {
        "id": "1b0b66ce"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime as dt\n",
        "import pyodbc\n",
        "import time\n",
        "\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "from azure.storage.blob import BlobClient\n",
        "from azure.storage.blob import BlobServiceClient, ContentSettings\n",
        "from azure.storage.blob import ContainerClient\n",
        "\n",
        "# Azure account info\n",
        "AZURE_BLOB_ACCOUNT_KEY = \"[INSERT ACCOUNT KEY HERE]\"\n",
        "AZURE_BLOB_ACCOUNT_URL = \"[INSERT ACCOUNT URL HERE]\"\n",
        "AZURE_FORM_RECOGNIZER_ENDPOINT = \"[INSERT FORMRECOGNIZER ENDPOINT HERE]\"\n",
        "AZURE_FORM_RECOGNIZER_KEY = \"[INSERT FORMRECOGNIZER KEY HERE]\"\n",
        "AZURE_BLOB_M187_CONTAINER = \"[INSERT CONTAINER NAME HERE]\"\n",
        "\n",
        "document_analysis_client = DocumentAnalysisClient(\n",
        "    endpoint=AZURE_FORM_RECOGNIZER_ENDPOINT, credential=AzureKeyCredential(AZURE_FORM_RECOGNIZER_KEY)\n",
        ")\n",
        "\n",
        "# model IDs\n",
        "AZURE_FORM_RECOGNIZER_CLASSIFIER_ID = \"[INSERT CLASSIFIER MODEL NAME HERE]\"\n",
        "BOLDFACE_MODEL_ID = \"[INSERT BOLDFACE EXTRACTION MODEL NAME HERE]\"\n",
        "NON_BOLDFACE_MODEL_ID = \"[INSERT NON-BOLDFACE EXTRACTION MODEL NAME HERE]\"\n",
        "\n",
        "# variable lists\n",
        "RAW_DATE_COL_LIST = [\"REPORT_DATE\", \"BIRTH_DATE\", \"SENTENCE_DATE\", \"DATE_ARREST\", \"DATE_CONVICTION\", \"DATE_OF_LAST_ADMISSION\", \"DEATH_DATE\", \"AUTOPSY_DATE\"]\n",
        "DATE_COL_LIST = [\"REPORT_DATE_D\", \"BIRTH_DATE_D\", \"SENTENCE_DATE_D\", \"DATE_ARREST_D\", \"DATE_CONVICTION_D\", \"DATE_OF_LAST_ADMISSION_D\", \"DEATH_DATE_D\", \"AUTOPSY_DATE_D\"]\n",
        "RAW_TIME_COL_LIST = [\"DEATH_TIME\", \"AUTOPSY_TIME\"]\n",
        "RAW_DATETIME_COL_LIST = [\"AUTOPSY_DATETIME\", \"DATETIME_ADMITTED\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94db411a",
      "metadata": {
        "id": "94db411a"
      },
      "outputs": [],
      "source": [
        "# reads names of M187 forms to process\n",
        "def read_file_names_from_blob_container():\n",
        "    '''\n",
        "    Returns the list of file names in an Azure blob container.\n",
        "    '''\n",
        "\n",
        "    # gen empty lists for filenames\n",
        "    FILE_LIST = []\n",
        "\n",
        "    # connect to M187 forms blob container\n",
        "    container = ContainerClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=AZURE_BLOB_M187_CONTAINER, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "\n",
        "    # make list of blobs in container\n",
        "    blob_list = container.list_blobs()\n",
        "\n",
        "    # organize file names of PDF files from blob container\n",
        "    for blob in blob_list:\n",
        "        if (blob.name).split('.')[-1].lower() == 'pdf':\n",
        "            FILE_LIST.append(blob.name)\n",
        "\n",
        "    return FILE_LIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07a93c0",
      "metadata": {
        "id": "b07a93c0"
      },
      "outputs": [],
      "source": [
        "def delete_blob_from_blob_container(blob_name):\n",
        "    '''\n",
        "    Deletes an Azure blob from Azure blob container.\n",
        "    '''\n",
        "\n",
        "    # connect to M187 blob container\n",
        "    container = ContainerClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=AZURE_BLOB_M187_CONTAINER, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "\n",
        "    # delete blob\n",
        "    container.delete_blob(blob=blob_name)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56a31a0",
      "metadata": {
        "id": "a56a31a0"
      },
      "outputs": [],
      "source": [
        "def save_csv_to_blob_container(df, file_name):\n",
        "    '''\n",
        "    Saves a Pandas DataFrame to an Azure blob container.\n",
        "    '''\n",
        "\n",
        "    # create the BlobServiceClient object\n",
        "    blob_service_client = BlobServiceClient(AZURE_BLOB_ACCOUNT_URL, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "    blob_client = blob_service_client.get_blob_client(container=AZURE_BLOB_M187_CONTAINER, blob=file_name)\n",
        "    blob_settings = ContentSettings(content_encoding='UTF-8')\n",
        "\n",
        "    # save file as blob to container, cool storage\n",
        "    blob_client.upload_blob(df.to_csv(encoding='UTF-8', index=False, sep='|'),overwrite=True,content_type=\"text/csv\")\n",
        "    blob_client.set_standard_blob_tier('Cool')\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbdc584",
      "metadata": {
        "id": "1fbdc584"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The following functions classify and extract fields from M187s using custom Azure Document Intelligence models.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a98bcf4",
      "metadata": {
        "id": "2a98bcf4"
      },
      "outputs": [],
      "source": [
        "def classify_documents(file_list):\n",
        "    '''\n",
        "    Classifies each page as (1) boldface first-page, (2) non-boldface first page, or (3) second page and returns dictionary\n",
        "    with document type corresponding to each filename. Note that documents need to be split into individual pages\n",
        "    before using this pipeline.\n",
        "    '''\n",
        "    classifier_id = os.getenv(\"CLASSIFIER_ID\", AZURE_FORM_RECOGNIZER_CLASSIFIER_ID)\n",
        "\n",
        "    file_types = {}\n",
        "\n",
        "    for file in file_list:\n",
        "        blob_client = BlobClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=\"m187-doccs-forms-blob\", blob_name=file, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "        poller = document_analysis_client.begin_classify_document_from_url(\n",
        "             classifier_id, document_url=blob_client.url\n",
        "         )\n",
        "        result = poller.result()\n",
        "\n",
        "        for page in result.documents: #documents are single-page\n",
        "            file_types[file] = page.doc_type\n",
        "    return file_types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b869e5",
      "metadata": {
        "id": "f1b869e5"
      },
      "outputs": [],
      "source": [
        "def extract_info_from_boldface_form(file, fields_values_df):\n",
        "    '''\n",
        "    Uses a custom Azure Document Intelligence model to extract fields from boldface forms (first page only).\n",
        "    '''\n",
        "\n",
        "    model_id = os.getenv(\"BOLDFACE_MODEL_ID\", BOLDFACE_MODEL_ID)\n",
        "    blob_client = BlobClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=\"m187-doccs-forms-blob\", blob_name=file, credential=AZURE_BLOB_ACCOUNT_KEY)[ ]\n",
        "\n",
        "\n",
        "    poller = document_analysis_client.begin_analyze_document_from_url(\n",
        "             model_id = BOLDFACE_MODEL_ID, document_url=blob_client.url\n",
        "         )\n",
        "    result = poller.result()\n",
        "\n",
        "    key_value_dict = {}\n",
        "    key_confidence_dict = {}\n",
        "\n",
        "    for document in result.documents:\n",
        "        key_value_dict[\"FORM_TYPE\"] = \"boldface\"\n",
        "        key_value_dict[\"FORM_URL\"] = blob_client.url\n",
        "        for field_name, field in document.fields.items():\n",
        "            key_value_dict[field_name] = field.value\n",
        "            key_confidence_dict[field_name] = field.confidence\n",
        "        key_value_dict[\"FIELD_CONFIDENCE_DICT\"] = key_confidence_dict\n",
        "        temp_df = pd.DataFrame([key_value_dict])\n",
        "        fields_values_df = pd.concat([fields_values_df, temp_df], ignore_index=True)\n",
        "\n",
        "    return fields_values_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944c585b",
      "metadata": {
        "id": "944c585b"
      },
      "outputs": [],
      "source": [
        "def extract_info_from_non_boldface_form(file, fields_values_df):\n",
        "    '''\n",
        "    Uses a custom Azure Document Intelligence model to extract fields from non-boldface forms (first page only).\n",
        "    '''\n",
        "\n",
        "    model_id = os.getenv(\"NON_BOLDFACE_MODEL_ID\", NON_BOLDFACE_MODEL_ID)\n",
        "    blob_client = BlobClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=\"m187-doccs-forms-blob\", blob_name=file, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "\n",
        "    poller = document_analysis_client.begin_analyze_document_from_url(\n",
        "             model_id = NON_BOLDFACE_MODEL_ID, document_url=blob_client.url\n",
        "         )\n",
        "    result = poller.result()\n",
        "\n",
        "    key_value_dict = {}\n",
        "    key_confidence_dict = {}\n",
        "\n",
        "    for document in result.documents:\n",
        "        key_value_dict[\"FORM_TYPE\"] = \"non-boldface\"\n",
        "        key_value_dict[\"FORM_URL\"] = blob_client.url\n",
        "        for field_name, field in document.fields.items():\n",
        "                key_value_dict[field_name] = field.value\n",
        "                key_confidence_dict[field_name] = field.confidence\n",
        "        key_value_dict[\"FIELD_CONFIDENCE_DICT\"] = key_confidence_dict\n",
        "        temp_df = pd.DataFrame([key_value_dict])\n",
        "        fields_values_df = pd.concat([fields_values_df, temp_df], ignore_index=True)\n",
        "\n",
        "    return fields_values_df\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5df3ba",
      "metadata": {
        "id": "de5df3ba"
      },
      "outputs": [],
      "source": [
        "def classify_and_extract_info(file_list, only_process_new_files, current_fields_values_df = None):\n",
        "    '''\n",
        "    Uses classify_documents() to determine the layout of each page and then uses extract_info_from_boldface_form()\n",
        "    and extract_info_from_non_boldface_form() to extract fields from first pages. Returns dataframe with extracted\n",
        "    fields.\n",
        "    '''\n",
        "\n",
        "    # classify first pages of forms as either boldface or non-boldface so we know which model to use on them\n",
        "    print('Starting classification')\n",
        "    classified_files_dict = classify_documents(file_list)\n",
        "    print('Classification step completed')\n",
        "\n",
        "    # if we want to process all files, create a new empty dataframe for fields and values\n",
        "    if only_process_new_files == False:\n",
        "        fields_values_df = pd.DataFrame(columns = [\"POTENTIAL_DUPLICATE\", \"FILE_NAME\", \"FORM_TYPE\", \"FORM_URL\", \"MANUAL_REVIEW_DONE\", \"DECEASED_NAME\", \"DIN\", \"NYSID\", \"FACILITY_NAME\", \"CODE\", \\\n",
        "        \"REPORT_DATE\", \"REPORTING_OFFICIAL_NAME\", \"HEIGHT\", \"HEIGHT_FT\", \"HEIGHT_IN\", \"ETHNICITY\", \"WEIGHT\", \"RACE\", \"SEX\", \\\n",
        "        \"SENTENCE\", \"BIRTH_DATE\", \"SENTENCE_DATE\", \"ARREST_CHARGES\", \"DATE_ARREST\", \"DATE_CONVICTION\", \"CONVICTION_CHARGES\", \\\n",
        "        \"HOSPITAL_NAME\", \"CHIEF_ADMIN_OFFICER_NAME\", \"AMBULANCE_RESCUE_SQUAD_NAME\", \"DATETIME_ADMITTED\", \"DATE_OF_LAST_ADMISSION\", \\\n",
        "        \"DEATH_DATE\", \"DEATH_TIME\", \"TERMINAL_INCIDENT_LOCATION\", \"REPORTED_IMMEDIATE_CAUSE_OF_DEATH\", \"FACILITY_ADMINISTRATORS_REPORT_OF_DEATH_CIRCUMSTANCES\", \\\n",
        "        \"STAFF_INCARCERATED_WITNESSES\", \"OFFICER_SUPERVISING_DEATH_LOCATION\", \"ASSIGNED_HOUSING_UNIT\", \"HOUSING_UNIT_TYPE\", \\\n",
        "        \"AUTOPSY_DATE\", \"AUTOPSY_TIME\", \"AUTOPSY_DATETIME\", \"AUTOPSY_LOCATION\", \"MEDICAL_EXAMINER_CORONER_NAME\", \"AUTOPSY_PERFORMED_YES\", \"AUTOPSY_PERFORMED_NO\", \\\n",
        "        \"SUPERVISION_PRIOR_TO_INCIDENT_ACTIVE\", \"SUPERVISION_PRIOR_TO_INCIDENT_CONSTANT\", \"SUPERVISION_PRIOR_TO_INCIDENT_GENERAL\",\n",
        "        \"SUBSTANCE_ABUSE_DRUG\", \"SUBSTANCE_ABUSE_ALCOHOL\", \"SUBSTANCE_ABUSE_UNKNOWN\", \"SUBSTANCE_ABUSE_NO\", \\\n",
        "        \"MEDICAL_TREATMENT\", \"PSYCH_TREATMENT\", \"NO_TREATMENT\", \"DATE_LAST_CONTACT\", \"MEDICAL_CONTACT\", \"PSYCH_CONTACT\", \\\n",
        "        \"INTAKE_SCREENING_YES\", \"INTAKE_SCREENING_NO\", \"FIELD_CONFIDENCE_DICT\", \"FACILITY_CODE_2D\", \"WEIGHT_LBS_F\", \"DATE_OF_BIRTH_D\", \\\n",
        "        \"DEATH_TIME_T\", \"DATE_ARREST_D\", \"DATE_CONVICTION_D\", \"SENTENCE_DATE_D\", \"AUTOPSY_DATE_D\", \"DATETIME_ADMITTED_DT\", \"DATE_OF_LAST_ADMISSION_D\"])\n",
        "\n",
        "    # if we only want to process new values, set fields_values_df to current dataframe so we can append to it\n",
        "    elif only_process_new_files == True:\n",
        "        fields_values_df = current_fields_values_df\n",
        "\n",
        "    # run field extraction models on each forms\n",
        "    for file in file_list:\n",
        "        if classified_files_dict[file] == \"boldface first page\":\n",
        "            fields_values_df = extract_info_from_boldface_form(file, fields_values_df)\n",
        "        elif classified_files_dict[file] == \"non-boldface first page\":\n",
        "            fields_values_df = extract_info_from_non_boldface_form(file, fields_values_df)\n",
        "\n",
        "    return fields_values_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d30c76e",
      "metadata": {
        "id": "3d30c76e"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The following functions standardize and clean the data once it has been extracted using the Document Intelligence models.\n",
        "They are called once prior to the manual review, and once afterwards.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c31ae3",
      "metadata": {
        "id": "f9c31ae3"
      },
      "outputs": [],
      "source": [
        "def clean_doccs_facility_name(df):\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ADIRONDACK'), 'FACILITY_NAME'] = 'ADIRONDACK'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ALBION'), 'FACILITY_NAME'] = 'ALBION'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ALTONA'), 'FACILITY_NAME'] = 'ALTONA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ARTHUR KILL'), 'FACILITY_NAME'] = 'ARTHUR KILL'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ATTICA'), 'FACILITY_NAME'] = 'ATTICA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('AUBURN'), 'FACILITY_NAME'] = 'AUBURN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BAYVIEW'), 'FACILITY_NAME'] = 'BAYVIEW'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BARE HILL'), 'FACILITY_NAME'] = 'BARE HILL'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BEACON'), 'FACILITY_NAME'] = 'BEACON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BEDFORD HILLS'), 'FACILITY_NAME'] = 'BEDFORD HILLS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BUFFALO'), 'FACILITY_NAME'] = 'BUFFALO'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BUTLER'), 'FACILITY_NAME'] = 'BUTLER'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP GABRIELS'), 'FACILITY_NAME'] = 'CAMP GABRIELS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP GEORGETOWN'), 'FACILITY_NAME'] = 'CAMP GEORGETOWN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP PHARSALIA'), 'FACILITY_NAME'] = 'CAMP PHARSALIA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAPE VINCENT'), 'FACILITY_NAME'] = 'CAPE VINCENT'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAYUGA'), 'FACILITY_NAME'] = 'CAYUGA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CHATEAUGAY'), 'FACILITY_NAME'] = 'CHATEAUGAY'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CLINTON'), 'FACILITY_NAME'] = 'CLINTON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('COLLINS'), 'FACILITY_NAME'] = 'COLLINS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('COXSACKIE'), 'FACILITY_NAME'] = 'COXSACKIE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('DOWNSTATE'), 'FACILITY_NAME'] = 'DOWNSTATE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('EASTERN'), 'FACILITY_NAME'] = 'EASTERN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('EDGECOMBE'), 'FACILITY_NAME'] = 'EDGECOMBE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ELMIRA'), 'FACILITY_NAME'] = 'ELMIRA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FISHKILL'), 'FACILITY_NAME'] = 'FISHKILL'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FIVE POINTS'), 'FACILITY_NAME'] = 'FIVE POINTS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FRANKLIN'), 'FACILITY_NAME'] = 'FRANKLIN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FULTON'), 'FACILITY_NAME'] = 'FULTON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GOWANDA'), 'FACILITY_NAME'] = 'GOWANDA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GOUVERNEUR'), 'FACILITY_NAME'] = 'GOUVERNEUR'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREAT MEADOW'), 'FACILITY_NAME'] = 'GREAT MEADOW'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREEN HAVEN'), 'FACILITY_NAME'] = 'GREEN HAVEN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREENE'), 'FACILITY_NAME'] = 'GREENE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GROVELAND'), 'FACILITY_NAME'] = 'GROVELAND'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('HALE CREEK'), 'FACILITY_NAME'] = 'HALE CREEK'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('HUDSON'), 'FACILITY_NAME'] = 'HUDSON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LAKEVIEW'), 'FACILITY_NAME'] = 'LAKEVIEW'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LINCOLN'), 'FACILITY_NAME'] = 'LINCOLN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LIVINGSTON'), 'FACILITY_NAME'] = 'LIVINGSTON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LYON MOUNTAIN'), 'FACILITY_NAME'] = 'LYON MOUNTAIN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MARCY'), 'FACILITY_NAME'] = 'MARCY'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MID-ORANGE'), 'FACILITY_NAME'] = 'MID-ORANGE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MID-STATE'), 'FACILITY_NAME'] = 'MID-STATE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MOHAWK'), 'FACILITY_NAME'] = 'MOHAWK'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MONTEREY'), 'FACILITY_NAME'] = 'MONTEREY'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MORIAH'), 'FACILITY_NAME'] = 'MORIAH'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MOUNT MCGREGOR'), 'FACILITY_NAME'] = 'MOUNT MCGREGOR'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('OGDENSBURG'), 'FACILITY_NAME'] = 'OGDENSBURG'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ONEIDA'), 'FACILITY_NAME'] = 'ONEIDA'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ORLEANS'), 'FACILITY_NAME'] = 'ORLEANS'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('OTISVILLE'), 'FACILITY_NAME'] = 'OTISVILLE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('QUEENSBORO'), 'FACILITY_NAME'] = 'QUEENSBORO'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('RIVERVIEW'), 'FACILITY_NAME'] = 'RIVERVIEW'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ROCHESTER'), 'FACILITY_NAME'] = 'ROCHESTER'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SHAWANGUNK'), 'FACILITY_NAME'] = 'SHAWANGUNK'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SING SING'), 'FACILITY_NAME'] = 'SING SING'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SOUTHPORT'), 'FACILITY_NAME'] = 'SOUTHPORT'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SULLIVAN'), 'FACILITY_NAME'] = 'SULLIVAN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SUMMIT'), 'FACILITY_NAME'] = 'SUMMIT'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('TACONIC'), 'FACILITY_NAME'] = 'TACONIC'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ULSTER'), 'FACILITY_NAME'] = 'ULSTER'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('UPSTATE'), 'FACILITY_NAME'] = 'UPSTATE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WALSH'), 'FACILITY_NAME'] = 'WALSH'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WALLKILL'), 'FACILITY_NAME'] = 'WALLKILL'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WATERTOWN'), 'FACILITY_NAME'] = 'WATERTOWN'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WASHINGTON'), 'FACILITY_NAME'] = 'WASHINGTON'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WENDE'), 'FACILITY_NAME'] = 'WENDE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WILLARD'), 'FACILITY_NAME'] = 'WILLARD'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WOODBOURNE'), 'FACILITY_NAME'] = 'WOODBOURNE'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WYOMING'), 'FACILITY_NAME'] = 'WYOMING'\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa09f6f",
      "metadata": {
        "id": "9fa09f6f"
      },
      "outputs": [],
      "source": [
        "def create_facility_code_2d(df):\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ADIRONDACK'), 'FACILITY_CODE_2D'] = '23'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ALBION'), 'FACILITY_CODE_2D'] = '09'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ALTONA'), 'FACILITY_CODE_2D'] = '54'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ARTHUR KILL'), 'FACILITY_CODE_2D'] = '15'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ATTICA'), 'FACILITY_CODE_2D'] = '00'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('AUBURN'), 'FACILITY_CODE_2D'] = '01'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BARE HILL'), 'FACILITY_CODE_2D'] = '56'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BAYVIEW'), 'FACILITY_CODE_2D'] = '31'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BEACON'), 'FACILITY_CODE_2D'] = '34'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BEDFORD HILLS'), 'FACILITY_CODE_2D'] = '12'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BUFFALO'), 'FACILITY_CODE_2D'] = '88'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('BUTLER'), 'FACILITY_CODE_2D'] = '52'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP GABRIELS'), 'FACILITY_CODE_2D'] = '22'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP GEORGETOWN'), 'FACILITY_CODE_2D'] = '21'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAMP PHARSALIA'), 'FACILITY_CODE_2D'] = '18'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAPE VINCENT'), 'FACILITY_CODE_2D'] = '58'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CAYUGA'), 'FACILITY_CODE_2D'] = '55'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CHATEAUGAY'), 'FACILITY_CODE_2D'] = '86'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('CLINTON'), 'FACILITY_CODE_2D'] = '02'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('COLLINS'), 'FACILITY_CODE_2D'] = '47'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('COXSACKIE'), 'FACILITY_CODE_2D'] = '13'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('DOWNSTATE'), 'FACILITY_CODE_2D'] = '24'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('EASTERN'), 'FACILITY_CODE_2D'] = '10'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('EDGECOMBE'), 'FACILITY_CODE_2D'] = '32'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ELMIRA'), 'FACILITY_CODE_2D'] = '11'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FISHKILL'), 'FACILITY_CODE_2D'] = '05'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FIVE POINT'), 'FACILITY_CODE_2D'] = '37'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FRANKLIN'), 'FACILITY_CODE_2D'] = '53'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('FULTON'), 'FACILITY_CODE_2D'] = '38'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GOWANDA'), 'FACILITY_CODE_2D'] = '45'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GOUVERNEUR'), 'FACILITY_CODE_2D'] = '81'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREAT MEADOW'), 'FACILITY_CODE_2D'] = '04'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREEN HAVEN'), 'FACILITY_CODE_2D'] = '08'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GREENE'), 'FACILITY_CODE_2D'] = '67'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('GROVELAND'), 'FACILITY_CODE_2D'] = '46'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('HALE CREEK'), 'FACILITY_CODE_2D'] = '85'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('HUDSON'), 'FACILITY_CODE_2D'] = '27'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LAKEVIEW'), 'FACILITY_CODE_2D'] = '60'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LINCOLN'), 'FACILITY_CODE_2D'] = '36'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LIVINGSTON'), 'FACILITY_CODE_2D'] = '80'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('LYON MOUNTAIN'), 'FACILITY_CODE_2D'] = '59'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MARCY'), 'FACILITY_CODE_2D'] = '49'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MID-ORANGE'), 'FACILITY_CODE_2D'] = '28'\n",
        "    df.loc[(df['FACILITY_NAME'].str.contains('MIDSTATE'))|(df['FACILITY_NAME'].str.contains('MID-STATE')), 'FACILITY_CODE_2D'] = '48'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MOHAWK'), 'FACILITY_CODE_2D'] = '39'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WALSH'), 'FACILITY_CODE_2D'] = '39'  # make Walsh have same facility code as Mohawk\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MONTEREY'), 'FACILITY_CODE_2D'] = '19'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MORIAH'), 'FACILITY_CODE_2D'] = '51'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('MCGREGOR'), 'FACILITY_CODE_2D'] = '26'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('OGDENSBURG'), 'FACILITY_CODE_2D'] = '35'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ONEIDA'), 'FACILITY_CODE_2D'] = '44'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ORLEANS'), 'FACILITY_CODE_2D'] = '64'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('OTISVILLE'), 'FACILITY_CODE_2D'] = '29'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('QUEENSBORO'), 'FACILITY_CODE_2D'] = '17'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('RIVERVIEW'), 'FACILITY_CODE_2D'] = '57'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ROCHESTER'), 'FACILITY_CODE_2D'] = '30'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SHAWANGUNK'), 'FACILITY_CODE_2D'] = '68'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SING SING'), 'FACILITY_CODE_2D'] = '07'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SOUTHPORT'), 'FACILITY_CODE_2D'] = '63'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SULLIVAN'), 'FACILITY_CODE_2D'] = '69'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('SUMMIT'), 'FACILITY_CODE_2D'] = '20'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('TACONIC'), 'FACILITY_CODE_2D'] = '25'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('ULSTER'), 'FACILITY_CODE_2D'] = '61'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('UPSTATE'), 'FACILITY_CODE_2D'] = '84'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WALLKILL'), 'FACILITY_CODE_2D'] = '06'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WATERTOWN'), 'FACILITY_CODE_2D'] = '03'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WASHINGTON'), 'FACILITY_CODE_2D'] = '65'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WENDE'), 'FACILITY_CODE_2D'] = '43'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WILLARD'), 'FACILITY_CODE_2D'] = '82'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WOODBOURNE'), 'FACILITY_CODE_2D'] = '14'\n",
        "    df.loc[df['FACILITY_NAME'].str.contains('WYOMING'), 'FACILITY_CODE_2D'] = '66'\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7ff67e",
      "metadata": {
        "id": "5f7ff67e"
      },
      "outputs": [],
      "source": [
        "def clean_race_col_create_ethnicity_col(df):\n",
        "\n",
        "    df.loc[df[\"RACE\"].str.contains(\"HISP\") & (df['ETHNICITY'] == \"\"), \"ETHNICITY\"] = \"HISPANIC\"\n",
        "    df.loc[df[\"RACE\"].str.contains(\"NHP\") & (df['ETHNICITY'] == \"\"), \"ETHNICITY\"] = \"NOT HISPANIC\"\n",
        "    df.loc[df[\"RACE\"].str.contains(\"NOT HISP\") & (df['ETHNICITY'] == \"\"), \"ETHNICITY\"] = \"NOT HISPANIC\"\n",
        "\n",
        "    df[\"RACE\"] = df[\"RACE\"].replace(\"AA\", \"Black\")\n",
        "    df.loc[df[\"RACE\"].str.contains(\"B\"), \"RACE\"] = \"BLACK\"\n",
        "    df.loc[df[\"RACE\"].str.contains(\"CAUC\"), \"RACE\"] = \"WHITE\" # caucasian\n",
        "    df.loc[df[\"RACE\"].str.contains(\"W\"), \"RACE\"] = \"WHITE\"\n",
        "    df.loc[df[\"RACE\"].str.contains(\"O\"), \"RACE\"] = \"OTHER\"\n",
        "    df[\"RACE\"] = df[\"RACE\"].replace(\"NHP\", \"\")\n",
        "    df[\"RACE\"] = df[\"RACE\"].replace(\"HISPANIC\", \"\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c2bfe9",
      "metadata": {
        "id": "c7c2bfe9"
      },
      "outputs": [],
      "source": [
        "def clean_date_time_cols(df):\n",
        "\n",
        "    for col in RAW_DATE_COL_LIST:\n",
        "        # clean and standardize date formatting\n",
        "        df[col] = df[col].str.replace('//', '/', regex=False)\n",
        "        df[col] = df[col].str.replace('/022', '/2022', regex=False)\n",
        "        df[col] = df[col].str.replace('-', '/', regex=False)\n",
        "        df[col] = df[col].apply(pd.to_datetime, errors='ignore').astype(str)\n",
        "\n",
        "        # remove time from datetime and ensure dates are in correct century\n",
        "        df[col] = df[col].astype(str)\n",
        "        df[col] = df[col].str.replace('00:00:00', '', regex=True)\n",
        "        df.loc[df[col].apply(lambda x: bool(re.match('^20([3-9])', x))), col] = '19' + df[col].str[2:]\n",
        "\n",
        "    for col in RAW_DATETIME_COL_LIST:\n",
        "        df[col] = df[col].apply(pd.to_datetime, errors='ignore').astype(str)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab9689b",
      "metadata": {
        "id": "5ab9689b"
      },
      "outputs": [],
      "source": [
        "def clean_remaining_cols(df):\n",
        "\n",
        "    # Name\n",
        "    df[\"DECEASED_NAME\"] = df[\"DECEASED_NAME\"].replace('[^a-zA-Z, ]', '', regex=True)\n",
        "\n",
        "    # DIN\n",
        "    df[\"DIN\"] = df[\"DIN\"].replace('[^a-zA-Z0-9]', '', regex=True)\n",
        "    df[\"DIN\"] = df[\"DIN\"].replace('NYSID', '', regex=True)\n",
        "    df[\"DIN\"] = df[\"DIN\"].str.upper()\n",
        "    df[\"DIN\"] = df[\"DIN\"].str.strip()\n",
        "\n",
        "    # height\n",
        "    df[\"HEIGHT_FT\"] = df[\"HEIGHT_FT\"].str.replace('O', '0', regex=True)\n",
        "    df[\"HEIGHT_IN\"] = df[\"HEIGHT_IN\"].str.replace('O', '0', regex=True)\n",
        "    df[\"HEIGHT_FT\"] = df[\"HEIGHT_FT\"].str.replace('[^0-9]', '', regex=True)\n",
        "    df[\"HEIGHT_IN\"] = df[\"HEIGHT_IN\"].str.replace('[^0-9]', '', regex=True)\n",
        "    df[\"HEIGHT_IN\"] = df[\"HEIGHT_IN\"].str.replace('17', '7', regex=True)\n",
        "\n",
        "    df[\"HEIGHT\"] = df[\"HEIGHT\"].str.replace('\\\"\\\"\\\"', '\\\"', regex=True)\n",
        "    df[\"HEIGHT\"] = df[\"HEIGHT\"].str.lstrip('\\\"')\n",
        "    df.loc[df[\"FORM_TYPE\"] == \"NON-BOLDFACE\", \"HEIGHT\"] = df[\"HEIGHT_FT\"] + '\\' ' + df[\"HEIGHT_IN\"] + '\\\"'\n",
        "    df[\"HEIGHT\"] = df[\"HEIGHT\"].str.replace('\\'[0-9]', '\\' ', regex=True)\n",
        "\n",
        "    df.loc[df[\"HEIGHT_FT\"] == \"0\", \"HEIGHT_IN\"] = \"\"\n",
        "    df.loc[df[\"HEIGHT_FT\"] == \"0\", \"HEIGHT_FT\"] = \"\"\n",
        "    df[\"HEIGHT\"] = df[\"HEIGHT\"].str.replace('\\' \\\"', '', regex=True)\n",
        "\n",
        "    # weight-- in lbs\n",
        "    df[\"WEIGHT\"] = df[\"WEIGHT\"].str.replace('LBS', '', regex=True)\n",
        "    df[\"HEIGHT\"] = df[\"HEIGHT\"].str.rstrip('.')\n",
        "\n",
        "    # sex\n",
        "    df['SEX'] = df['SEX'].str[:1]\n",
        "    df['SEX'] = df['SEX'].str.replace(\"N\", \"M\", regex = False)\n",
        "\n",
        "    # reported immediate cause of death\n",
        "    df[\"REPORTED_IMMEDIATE_CAUSE_OF_DEATH\"] = df[\"REPORTED_IMMEDIATE_CAUSE_OF_DEATH\"].str.replace('REPORTED IMMEDIATE CAUSE OF DEATH:', '', regex=True)\n",
        "\n",
        "    # staff_incarcerated_witnesses\n",
        "    df[\"STAFF_INCARCERATED_WITNESSES\"] = df[\"STAFF_INCARCERATED_WITNESSES\"].str.replace('DOCCS :', '', regex = True)\n",
        "\n",
        "    # sentence\n",
        "    df[\"SENTENCE\"] = df[\"SENTENCE\"].str.replace(\"YRS\", \"YEARS\", regex = True)\n",
        "    df[\"SENTENCE\"] = df[\"SENTENCE\"].str.replace(\";\", \", \", regex = True)\n",
        "\n",
        "    # hospital name\n",
        "    df[\"HOSPITAL_NAME\"] = df[\"HOSPITAL_NAME\"].str.replace('Hospital:', '', regex=True)\n",
        "\n",
        "    # housing unit abbreviations\n",
        "    df = df.replace({'REGIONAL MEDICAL UNIT': 'RMU'}, regex=True)\n",
        "    df = df.replace({'GP': 'GENERAL POPULATION'}, regex=False)\n",
        "    df.loc[df['HOUSING_UNIT_TYPE'].str.contains('GEN'), 'HOUSING_UNIT_TYPE'] = 'GENERAL POPULATION'\n",
        "    df[\"ASSIGNED_HOUSING_UNIT\"] = df[\"ASSIGNED_HOUSING_UNIT\"].str.replace(\" - \", \"-\", regex = True)\n",
        "\n",
        "    # form URL -- make the appropriate phrases lowercase so the PDF will download properly\n",
        "    df[\"FORM_URL\"] = df[\"FORM_URL\"].str.replace(\".PDF\", \".pdf\", regex = True)\n",
        "    df[\"FORM_URL\"] = df[\"FORM_URL\"].str.replace(\"M187-DOCCS-FORMS-BLOB\", \"m187-doccs-forms-blob\", regex = True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd04f68",
      "metadata": {
        "id": "9bd04f68"
      },
      "outputs": [],
      "source": [
        "def flag_duplicates(df):\n",
        "    '''\n",
        "    Flags potential duplicate forms based on whether the name on the form is the same as a name on another form.\n",
        "    During the manual review, users will need to delete all but one form for each duplicate, ideally\n",
        "    keeping the form with the most information (some duplicate names correspond with different forms).\n",
        "    '''\n",
        "    df[\"POTENTIAL_DUPLICATE\"] = df.duplicated(keep=False, subset=['DECEASED_NAME'])\n",
        "    df[\"POTENTIAL_DUPLICATE\"] = df[\"POTENTIAL_DUPLICATE\"].astype(str)\n",
        "    df[\"POTENTIAL_DUPLICATE\"] = df[\"POTENTIAL_DUPLICATE\"].str.upper()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21950ad",
      "metadata": {
        "id": "c21950ad"
      },
      "outputs": [],
      "source": [
        "def clean_df(df):\n",
        "    '''\n",
        "    Runs all data cleaning and standardization functions, as well as flags potential duplicates using flag_duplicates().\n",
        "    '''\n",
        "    df = df.astype(str)\n",
        "    df = df.dropna(how='all')\n",
        "    df = df.replace({'nan': ''}, regex=True)\n",
        "    df = df.replace({'|': ''}, regex=True)\n",
        "    df = df.replace({'\\r': ''}, regex=True)\n",
        "    df = df.replace({'DNA': 'DID NOT ANSWER'}, regex=False)\n",
        "\n",
        "    #df = to_upper(df)\n",
        "    df = df.applymap(lambda s: s.upper() if type(s) == str else s)\n",
        "\n",
        "    df = clean_doccs_facility_name(df)\n",
        "    df = clean_race_col_create_ethnicity_col(df)\n",
        "    df = clean_date_time_cols(df)\n",
        "    df = clean_remaining_cols(df)\n",
        "\n",
        "    #df = strip_whitespace(df)\n",
        "    df = df.applymap(lambda s: s.strip() if type(s) == str else s)\n",
        "\n",
        "    df = flag_duplicates(df)\n",
        "\n",
        "    df = df.astype(str)\n",
        "    df = df.replace({'NaT': ''}, regex=True)\n",
        "    df = df.replace({'NONE': ''}, regex=True)\n",
        "\n",
        "    # abbreviate checkbox indicators for ease of checking\n",
        "    df = df.replace({'UNSELECTED': ''}, regex=True)\n",
        "    df = df.replace({'SELECTED': 'X'}, regex=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "650a8ab2",
      "metadata": {
        "id": "650a8ab2"
      },
      "outputs": [],
      "source": [
        "def data_qa_m187s(df):\n",
        "    '''\n",
        "    Runs QA on fields after the manual review to check that data is within expected bounds.\n",
        "    '''\n",
        "\n",
        "    try:\n",
        "        # date columns\n",
        "        for col in DATE_COL_LIST:\n",
        "            assert (df[col] == '').any()==False                                                                                              , 'QA failure: Report date should never be missing'\n",
        "            assert pd.to_datetime(df[col]).dt.month.min() >= 1 & pd.to_datetime(df[col]).dt.month.min() <= 12                  , 'QA Failure: Report date month should always be between 1-12'\n",
        "            assert pd.to_datetime(df[col].max()).date() <= dt.date.today()                                                                  , 'QA failure: Report date can not be in the future'\n",
        "\n",
        "        # DIN\n",
        "        assert (df['DIN'] == \"REDACTED\") | (df['DIN'].apply(len) == 7).all()                                         ,'QA Failure: DIN should always be 7 digits and not missing'\n",
        "        assert (df['DIN'] == \"REDACTED\") | (df['DIN'].apply(lambda x: x.isalnum()).all() == True)                      ,'QA Failure: DIN should always be alphanumeric'\n",
        "        assert (df['DIN'] == \"REDACTED\") | (df['DIN'].str[0:2].apply(lambda x: x.isnumeric()).all() == True)           ,'QA Failure: first two DIN digits should be numeric'\n",
        "        assert (df['DIN'] == \"REDACTED\") | (df['DIN'].str[2].isin(['A','B','C','D','E','G','H','I','J','N','P','R','S','T','X','Y']).all() == True),'QA Failure: third DIN digit should be from set of letters used to indicate reception facility'\n",
        "        assert (df['DIN'] == \"REDACTED\") | (df['DIN'].str[3:].apply(lambda x: x.isnumeric()).all() == True)            ,'QA Failure: last 4 DIN digits should be numeric'\n",
        "\n",
        "        # sex\n",
        "        sex_codes = ['M','F','', 'REDACTED']\n",
        "        assert df['SEX_CODE'].isin(sex_codes).all() == True                              , 'QA failure: sex code should be in the predefined set'\n",
        "\n",
        "        # race\n",
        "        race_codes = ['BLACK', 'WHITE', 'NATIVE AMERICAN', 'OTHER', 'REDACTED', '']\n",
        "        assert df['RACE_CODE'].isin(race_codes).all() == True                                                                                                                               , 'QA failure: race code should be in the predefined set'\n",
        "\n",
        "        # ethnicity\n",
        "        ethnic_group_codes = ['HISPANIC','NOT HISPANIC','UNKNOWN','REDACTED','']\n",
        "        assert df['ETHNIC_GROUP'].isin(ethnic_group_codes).all() == True                                                                        , 'QA failure: ethnic group code should be in the predefined set'\n",
        "\n",
        "        # weight\n",
        "        assert (df['WEIGHT_LBS_F'] < 0    ).any() == False                                                        , 'QA failure: weight should never be negative'\n",
        "\n",
        "        # facility code\n",
        "        facility_2D_codes = ['','23','09','54','15','00','01','56','31','34','12','88','52','22','21','18','58','55','86','02',\n",
        "                             '47','13','24','10','32','11','05','37','53','38','81','45','04','08','67','46','85','27','60',\n",
        "                             '36','80','59','49','28','48','39','19','51','26','35','44','64','29','17','57','30','68','07',\n",
        "                             '63','69','20','25','61','84','06','65','03','43','82','14','66']\n",
        "        assert df['FACILITY_2D_CODE'].isin(facility_2D_codes).all() == True                                        , 'QA failure: 2-digit facility code should be in the predefined set'\n",
        "\n",
        "    except Exception as error:\n",
        "        print(\"An exception occurred:\", error)\n",
        "        pdb.set_trace()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4cf93f",
      "metadata": {
        "id": "fa4cf93f"
      },
      "outputs": [],
      "source": [
        "def process_raw_forms(only_process_new_files):\n",
        "    '''\n",
        "    The main function for before the manual review step. Classifies and extracts information from forms and performs\n",
        "    preliminary data cleaning before uploading to the Azure blob as a CSV. Set only_process_new_files to True if we\n",
        "    only want to process M187s that haven't been processed yet; set to False if we want to drop all processed data\n",
        "    and reprocess all forms.\n",
        "    '''\n",
        "\n",
        "    # read file names from blob container\n",
        "    FILE_LIST = read_file_names_from_blob_container()\n",
        "\n",
        "    print('Files to process:', FILE_LIST)\n",
        "\n",
        "    # delete old extracted fields df so we can upload a new one later\n",
        "    delete_blob_from_blob_container(\"M187_DOCCS_extracted_fields_table.csv\")\n",
        "\n",
        "    # classify and extract information from forms\n",
        "    df = classify_and_extract_info(FILE_LIST, only_process_new_files)\n",
        "\n",
        "    # clean data\n",
        "    df = clean_df(df)\n",
        "\n",
        "    # set manual review field to \"no\"\n",
        "    df[\"MANUAL_REVIEW_DONE\"] = \"NO\"\n",
        "\n",
        "    # save csv to Azure blob\n",
        "    save_csv_to_blob_container(df, \"M187_DOCCS_extracted_fields_table.csv\")\n",
        "\n",
        "    print('Processing complete')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43110144",
      "metadata": {
        "id": "43110144"
      },
      "outputs": [],
      "source": [
        "def clean_data_post_processing():\n",
        "    '''\n",
        "    The main function for after the manual review step. Performs data QA and final data cleaning before uploading the\n",
        "    cleaned file to the Azure blob as a CSV.\n",
        "    '''\n",
        "    file = \"M187_DOCCS_extracted_fields_table.csv\"\n",
        "    blob_client = BlobClient(account_url=AZURE_BLOB_ACCOUNT_URL, container_name=\"m187-doccs-forms-blob\", blob_name=file, credential=AZURE_BLOB_ACCOUNT_KEY)\n",
        "    blob_download = blob_client.download_blob()\n",
        "    blob_content = blob_download.readall().decode('UTF-8')\n",
        "    df = pd.DataFrame([x.split('|') for x in blob_content.split('\\n')])\n",
        "\n",
        "    # make top row header\n",
        "    df = df.replace({'\\r': ''}, regex=True)\n",
        "    header = df.iloc[0]\n",
        "    df = df[1:]\n",
        "    df.columns = header\n",
        "\n",
        "    # final clean of dataframe\n",
        "    df = clean_df(df)\n",
        "\n",
        "    # create cleaned versions of the following: facility code 2d, weight, date and datetime columns\n",
        "    df = create_facility_code_2d(df)\n",
        "    df[\"WEIGHT_LBS_F\"] = pd.to_numeric(df[\"WEIGHT\"], errors='coerce')\n",
        "    df[\"WEIGHT_LBS_F\"] = df[\"WEIGHT_LBS_F\"].replace(np.nan, '', regex=True)\n",
        "    df[\"DATE_OF_BIRTH_D\"] = pd.to_datetime(df[\"BIRTH_DATE\"], errors='coerce')\n",
        "    df[\"DATE_ARREST_D\"] = pd.to_datetime(df[\"DATE_ARREST\"], errors='coerce')\n",
        "    df[\"DATE_CONVICTION_D\"] = pd.to_datetime(df[\"DATE_CONVICTION\"], errors='coerce')\n",
        "    df[\"SENTENCE_DATE_D\"] = pd.to_datetime(df[\"SENTENCE_DATE\"], errors='coerce')\n",
        "    df[\"AUTOPSY_DATE_D\"] = pd.to_datetime(df[\"AUTOPSY_DATE\"], errors='coerce')\n",
        "    df[\"DATETIME_ADMITTED_DT\"] = pd.to_datetime(df[\"DATETIME_ADMITTED\"], errors='coerce')\n",
        "    df[\"DATE_OF_LAST_ADMISSION_D\"] = pd.to_datetime(df[\"DATE_OF_LAST_ADMISSION\"], errors='coerce')\n",
        "\n",
        "    # create cleaned version of death time column: remove ambiguous time stamps (1:00 to 12:59 with no AM/PM indicator), convert remaining times to 24 hr\n",
        "    df[\"DEATH_TIME_T\"] = df[\"DEATH_TIME\"]\n",
        "    df[\"DEATH_TIME_T\"] = df[\"DEATH_TIME_T\"].str.replace(\":\", \"\", regex = True)\n",
        "    df[\"DEATH_TIME_T\"] = pd.to_numeric(df[\"DEATH_TIME_T\"], errors = 'coerce')\n",
        "    df.loc[(df[\"DEATH_TIME_T\"]> 99) & (df[\"DEATH_TIME_T\"] < 1300), \"DEATH_TIME_T\"] = \"\"\n",
        "    df.loc[df[\"DEATH_TIME_T\"] != \"\", \"DEATH_TIME_T\"] = df[\"DEATH_TIME\"]\n",
        "\n",
        "    # perform quality assurance\n",
        "    df = data_qa_m187s(df)\n",
        "\n",
        "    # delete the old blob from the container so we can upload the new dataframe\n",
        "    delete_blob_from_blob_container(\"M187_DOCCS_extracted_fields_table.csv\")\n",
        "\n",
        "    # save cleaned dataframe to container\n",
        "    save_csv_to_blob_container(df, \"M187_DOCCS_extracted_fields_table.csv\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f12df938",
      "metadata": {
        "id": "f12df938"
      },
      "outputs": [],
      "source": [
        "# main function execution\n",
        "def main(process_data, only_process_new_files, run_post_processing):\n",
        "\n",
        "    if process_data == True:\n",
        "\n",
        "        process_raw_forms(only_process_new_files)\n",
        "\n",
        "\n",
        "    if run_post_processing == True:\n",
        "\n",
        "        clean_data_post_processing()\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb627393",
      "metadata": {
        "id": "bb627393"
      },
      "outputs": [],
      "source": [
        "# run settings\n",
        "'''\n",
        "First set process_data to True and run_post_manual_review to False. Set only_process_new_files to either True or False depending on\n",
        "whether to process only new files, or to reprocess all files.\n",
        "\n",
        "Then, perform the MANUAL REVIEW STEP: Manually clean the output data to correct Document Intelligence errors. Flag redactions and keep only one\n",
        "duplicate for each set of duplicates.\n",
        "\n",
        "Finally, set process_data to False and run_post_manual_review to True. This will process the manually reviewed data to standardize the formatting.\n",
        "'''\n",
        "process_data = True # read and transform raw data files\n",
        "\n",
        "run_post_manual_review = False # run data processing after manual review process has been completed\n",
        "\n",
        "main(process_data, only_process_new_files, run_post_processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae424cc4",
      "metadata": {
        "id": "ae424cc4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}